##################################################
# install your miniconda

# conda create -n enb python=3.10
# conda activate enb

# pip install torch==2.6.0+cu124 torchvision==0.21.0 torchaudio==2.6.0 --index-url https://download.pytorch.org/whl/cu124
# conda install cudatoolkit=12.4 -c anaconda
# pip install --resume-retries 5 https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1+cu12torch2.6cxx11abiFALSE-cp310-cp310-linux_x86_64.whl
# pip install --resume-retries 5 https://github.com/flashinfer-ai/flashinfer/releases/download/v0.2.2.post1/flashinfer_python-0.2.2.post1+cu124torch2.6-cp38-abi3-linux_x86_64.whl

# pip install https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1+cu12torch2.6cxx11abiFALSE-cp311-cp311-linux_x86_64.whl

## or
# pip install psutil
# NVCC_APPEND_FLAGS='-allow-unsupported-compiler' FLASH_ATTENTION_SKIP_CUDA_BUILD=1 pip install flash-attn==2.7.4.post1 --no-build-isolation --use-pep517
# pip install flashinfer-python==0.2.2.post1 -i https://flashinfer.ai/whl/cu124/torch2.6/

# cd your_path_to_save_vllm-dev
# git clone https://github.com/Master-PLC/vllm-dev.git

## if failed with network
## cd to target dir and get head
## git ls-tree HEAD csrc/composable_kernel | awk '{print $3}'
## git clone xxx source_dir
## git checkout head
## cp -a source_dir target_dir
## cd target_dr.parent
## rm -rf target_dir/.git
## git submodule deinit -f target_dir || true
## git submodule absorbgitdirs target_dir
## req=$(git ls-tree HEAD target_dir | awk '{print $3}')
## git -C csrc/composable_kernel checkout "$req"

## 事实上，只要下面的pip install --no-build-isolation -e . 2>&1 | tee -a build.log，不是网络问题就是安装版cuda和系统cuda不匹配

# cd your_path_to_vllm-dev
#### below two steps are enough for system cuda==12.4
# pip install -r requirements/build.txt
# pip install --no-build-isolation -e .
### or
# pip install uv
# VLLM_USE_PRECOMPILED=1 uv pip install --no-build-isolation -e .
### or
# MAX_JOBS=4 pip install --no-build-isolation -e . 2>&1 | tee -a build.log
### or
# download https://wheels.vllm.ai/nightly/vllm-1.0.0.dev-cp38-abi3-manylinux1_x86_64.whl to somewhere
# VLLM_PRECOMPILED_WHEEL_LOCATION=somewhere/vllm-1.0.0.dev-cp38-abi3-manylinux1_x86_64.whl VLLM_USE_PRECOMPILED=1 pip install --no-build-isolation -e . 2>&1 | tee -a build.log
# pip uninstall opencv-python-headless

### maybe extra steps for other system cuda, however, we haven't validated
# python use_existing_torch.py
# pip install -r requirements/cuda.txt
# export CUDA_HOME=/usr/local/cuda

# cd your_path_to_save_llamafactory, hope for version near to 0.9.4.dev0
# git clone https://github.com/hiyouga/LLaMA-Factory.git
# cd LLaMA-Factory
# pip install -e ".[torch,metrics]" --no-build-isolation

## cd E-Prune
# pip install -r requirements.txt

## test gpu available
# python -c "import torch; print(torch.__version__); print(torch.cuda.is_available())"
##################################################

# core deps
transformers>=4.49.0,<=4.52.4,!=4.52.0; sys_platform != 'darwin'
transformers>=4.49.0,<=4.51.3,!=4.52.0; sys_platform == 'darwin'
datasets>=2.16.0,<=3.6.0
accelerate>=1.3.0,<=1.7.0
peft>=0.14.0,<=0.15.2
trl>=0.8.6,<=0.9.6
tokenizers>=0.19.0,<=0.21.1
# gui
gradio>=4.38.0,<=5.31.0
matplotlib>=3.7.0
tensorboard
tyro<0.9.0
# ops
einops
numpy<2.0.0
pandas>=2.0.0
pass-at-k
scipy
# model and tokenizer
sentencepiece
tiktoken
modelscope>=1.14.0
hf-transfer
# python
dotenv
fire
ipykernel
isort
omegaconf
packaging
protobuf
pyyaml
pydantic<=2.10.6
setproctitle
opencv-python-headless
ujson
# api
uvicorn
fastapi
modelscope
openai
sse-starlette
# media
av
librosa
openpyxl
