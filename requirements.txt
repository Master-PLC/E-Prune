##################################################
# install your miniconda

# conda create -n enb python=3.10
# conda activate enb

# pip install torch==2.6.0 torchvision==0.21.0 torchaudio==2.6.0 --index-url https://download.pytorch.org/whl/cu124
# conda install cudatoolkit=12.4 -c anaconda
# pip install --resume-retries 5 https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1+cu12torch2.6cxx11abiFALSE-cp310-cp310-linux_x86_64.whl
# pip install --resume-retries 5 https://github.com/flashinfer-ai/flashinfer/releases/download/v0.2.2.post1/flashinfer_python-0.2.2.post1+cu124torch2.6-cp38-abi3-linux_x86_64.whl


# pip install -r requirements.txt

# cd your_path_to_save_vllm-dev
# git clone https://github.com/Master-PLC/vllm-dev.git

# cd your_path_to_vllm-dev
#### below two steps are enough for system cuda==12.4
# pip install -r requirements/build.txt
# pip install --no-build-isolation -e .

### maybe extra steps for other system cuda, however, we haven't validated
# python use_existing_torch.py
# pip install -r requirements/cuda.txt
# export CUDA_HOME=/usr/local/cuda

## test gpu available
# python -c "import torch; print(torch.__version__); print(torch.cuda.is_available())"
##################################################

# core deps
transformers>=4.49.0,<=4.52.4,!=4.52.0; sys_platform != 'darwin'
transformers>=4.49.0,<=4.51.3,!=4.52.0; sys_platform == 'darwin'
datasets>=2.16.0,<=3.6.0
accelerate>=1.3.0,<=1.7.0
peft>=0.14.0,<=0.15.2
trl>=0.8.6,<=0.9.6
tokenizers>=0.19.0,<=0.21.1
# gui
gradio>=4.38.0,<=5.31.0
matplotlib>=3.7.0
tensorboard
tyro<0.9.0
# ops
einops
numpy<2.0.0
pandas>=2.0.0
pass-at-k
scipy
# model and tokenizer
sentencepiece
tiktoken
modelscope>=1.14.0
hf-transfer
# python
fire
ipykernel
isort
omegaconf
packaging
protobuf
pyyaml
pydantic<=2.10.6
setproctitle
ujson
# api
uvicorn
fastapi
modelscope
openai
sse-starlette
# media
av
librosa
openpyxl
